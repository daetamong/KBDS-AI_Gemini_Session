{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e16b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'Python 3.12.3'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m필요한 패키지를 사용하여 <a href='command:jupyter.createPythonEnvAndSelectController'>Python 환경 만들기</a>\n",
      "\u001b[1;31m또는 다음 명령을 사용하여 'ipykernel'을(를) 설치합니다. 'c:/Users/daewoo/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c97b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3560984",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GOOGLE_API_KEY\") # GEMINI_API_KEY 키에 해당하는 value를 가져온다 (=api key)\n",
    "print(api_key[:-5]) # 보안상의 이유로 끝의 5자리는 생략!|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6369d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key를 등록, 저장\n",
    "# 이 코드를 실행함으로써 Gemini 모델 객체를 만들거나 generate_content()와 같은 api 호출 메소드를 반복해서 실행할 때 매번 key를 넘기지 않아도 된다. -> 일종의 프로젝트 key 전역설정\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0257b",
   "metadata": {},
   "source": [
    "# Gemini API 가격 및 사용 가능 토큰 확인\n",
    "https://ai.google.dev/gemini-api/docs/pricing?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0edd15",
   "metadata": {},
   "source": [
    "### 현재 무료로 사용할 수 있는 Gemini 모델 종류\n",
    "\n",
    "1. Gemini 2.5 Flash\n",
    "2. Gemini 2.5 Flash-Lite\n",
    "3. Gemini 2.5 Pro\n",
    "4. Gemini Embeddings 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baab175",
   "metadata": {},
   "source": [
    "|모델|RPM (분당 요청)|TPM (분당 토큰)|RPD (일일 요청)|\n",
    "|--|--|--|--|\n",
    "|Gemini 2.5 Pro|5|250,000|100|\n",
    "|Gemini 2.5 Flash|10|250,000|250|\n",
    "|Gemini 2.5 Flash-Lite|15|250,000|1,500|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0be30a",
   "metadata": {},
   "source": [
    "# Chapter 1. Gemini에 직접 질문을 해서 답변을 받아보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77255c1f",
   "metadata": {},
   "source": [
    "## reponse : LLM 모델의 답변\n",
    "- response.text → 최종 생성 텍스트\n",
    "- response.candidates → 여러 후보 응답\n",
    "- response.usage_metadata → 토큰 사용량 정보\n",
    "\n",
    "etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. invoke(text) 내의 문장(text)을 프롬프트로 LLM 모델에게 전달\n",
    "# 2. invoke() : Google Gemini API 서버에 요청\n",
    "# 3. response : LLM 모델이 내놓은 답변 (여러 정보 확인 가능)\n",
    "response = model.invoke(\"안녕하세요, Gemini API 테스트 중입니다.\") # 질문 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 답변을 확인\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9b3c8",
   "metadata": {},
   "source": [
    "# Chapter 2. 우리의 대화를 기억하게 하자\n",
    "- 지금까지의 대화 세션을 저장, 메모리 캐싱해서 추가적인 질문 없이 Gemini가 직접 대화를 검색하도록 하는 방법\n",
    "> 동일한 질문에 대한 답변을 저장, 메모리에 저장된 답변을 반환\n",
    "- 토큰 사용량이 줄어듦\n",
    "- 대답 속도 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인메모리 캐시를 사용합니다.\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Gemini 모델 불러오기\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Gemini에 질문 입력하기 위한 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(\"{country}에 대해서 설명해줘. 최대한 짧게!\")\n",
    "\n",
    "# 프롬프트 생성부터 모델 불러오기, 답변 출력까지 하나의 체인으로 정의 -> 'chain'이라는 이름으로 만들어준다\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print('Gemini의 답변 : ', chain.invoke({\"country\": \"한국\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9afd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0eafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"미국\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21301ec",
   "metadata": {},
   "source": [
    "# Chapter 3. 우리가 원하는 포맷으로 답변을 받아보자!\n",
    "- Zero Shot 프롬프트 : 원하는 답변 포맷을 제공하지 않음\n",
    "- One Shot 프롬프트 : 원하는 답변 포맷을 한 가지 예시로 제공하는 방법\n",
    "- Few Shot 프롬프트 : 원하는 답변 포맷을 여러 예시로 제공하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78450006",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"최근 가장 급등한 주식 종목 알려줘\",\n",
    "\n",
    "        \"answer\": \"\"\"\n",
    "추가 질문: 대표적인 미국 주식 중 3개를 알려줘\n",
    "중간 답변: 1. 엔비디아 2. 구글 3.메타\n",
    "최종 답변은: 엔비디아, 테슬라, 팔란티어가 가장 많이 올랐습니다.\n",
    "\"\"\"},\n",
    "    {\n",
    "        \"question\": \"최근 가장 급등한 코인 알려줘\",\n",
    "\n",
    "        \"answer\": \"\"\"\n",
    "추가 질문: 대표적인 코인 3개를 알려줘\n",
    "중간 답변: 1. 비트코인 2. 이더리움 3. 도지\n",
    "최종 답변은: 비트코인, 이더리움, 리플이 가장 많이 올랐습니다.\n",
    "\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\n",
    "\"Question:\\n{question}\\nAnswer:\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b5c95",
   "metadata": {},
   "source": [
    "##### promt : 실제 LLM 모델이 받아드리는 질문 포맷 (학습 완료!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eedf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed35348",
   "metadata": {},
   "source": [
    "#### 첫번째, 두번째 Question, Answer : 우리가 원하는 질문에 따른 답변의 포맷\n",
    "#### 마지막 Question : 실제 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 질문\n",
    "question = \"최근 1년 동안 가장 많이 오른 한국 주식 알려줘\"\n",
    "final_prompt = prompt.format(question=question)\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be94dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d8115",
   "metadata": {},
   "source": [
    "# 주의사항\n",
    "- 예시 포맷도 토큰 비용에 포함되기 때문에 토큰 사용량을 고려한 적절한 처리가 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
